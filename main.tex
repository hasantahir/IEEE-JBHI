%% bare_jrnl_comsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.




% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[journal,comsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal,comsoc]{../sty/IEEEtran}

\newcommand\notsotiny{\@setfontsize\notsotiny{6}{7}}
\usepackage[T1]{fontenc}% optional T1 font encoding

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)

\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{amsfonts} % to load math symbols
\usepackage{commath} % For defining functions
\usepackage{mdwmath}
% \usepackage{showframe}
% \usepackage{mdwtab}
% \usepackage{minipage}
% \usepackage[keeplastbox]{flushend}
\usepackage{graphicx}
% \usepackage{etoolbox}
\usepackage{color}
% \usepackage{placeins}
% \usepackage{subfigure}
\usepackage{subcaption}
% \usepackage{hyperref}
% % The following is done to hide ugly color boxes around the links
\usepackage[table]{xcolor}
% \hypersetup{
% colorlinks,
% linkcolor={black},
% citecolor={black},
% urlcolor={black}
% }
\usepackage{booktabs}
\usepackage{float}
\usepackage{newtxtext,newtxmath}
% \usepackage{geometry} % just not to bother with the table width
% \usepackage{standalone}
% \usepackage{filecontents}
\usepackage{tabularx,colortbl}
\usepackage{pgfplots}
\usepackage{standalone}
\usepackage{tikz}
\tikzset{
font={\fontsize{8pt}{8}\selectfont}}
\usepackage{tikzscale} % Scale the figure not the font
\pgfplotsset{compat=newest}
%% the following commands are sometimes needed
\usepackage{grffile}
\usepackage{mathtools}
\usepackage[separate-uncertainty = true,
  multi-part-units = repeat]{siunitx}
\sisetup{per=slash, load=abbr, output-complex-root = j, complex-root-position = before-number}

% *** CITATION PACKAGES ***
%
% \usepackage{cite}
\usepackage[noadjust]{cite}


% *** GRAPHICS RELATED PACKAGES ***
%
% \usepackage{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{Figures/}}
%
% Personal definitions
\renewcommand{\v}[1]{\mathbf{#1}} % vectors
\newcommand{\ti}[1]{\tilde{#1}} % spectral representation
\newcommand{\tnsr}[1]{\underline{\underline{#1}}}
% Operators
\renewcommand{\O}{\omega}  % omega
\newcommand{\E}{\varepsilon}  % epsilon
\renewcommand{\u}{\mu}  % mu
\newcommand{\p}{\rho}  % rho
\newcommand{\vp}{\boldsymbol \p}  % rho
\newcommand{\x}{\times}  % times
\renewcommand{\inf}{\infty}  % infinity
\newcommand{\infint}{\int\limits_{-\inf}^\inf} % integral by R
\renewcommand{\del}{\nabla}  % nabla operator
\renewcommand{\^}{\hat}  % unit vector
\newcommand*\diff{\mathop{}\!\mathrm{d}} % Define differential operator
\newcommand{\e}{\mathrm{e}} % Straight-up exponential
\renewcommand{\j}{{j}\mkern1mu} % Straight-up exponential
\newcommand{\iu}{\mathrm{i}\mkern1mu}


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
\title{A Type 2 Diabetes Prognostic Model based on Support Vector Machine}
%
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Hasan~Abbas,~
Lejla~Alic,~
Shafiqul~Islam,~
Madhav~Erraguntla,~
Muhammad Abdul-Ghani,~
Qammer~Abbasi,~\IEEEmembership{Senior Member,~IEEE,}
and~Marwa~Qaraqe,~\IEEEmembership{Member,~IEEE}% <-this % stops a space
\thanks{H. Abbas is with the Department
of Electrical and Computer Engineering, Texas A\&M University at Qatar, Doha 23874 Qatar;  hasan.abbas@qatar.tamu.edu}% <-this % stops a space
\thanks{L. Alic was with the Department
of Electrical and Computer Engineering, Texas A\&M University at Qatar, Doha 23874 Qatar; lejlaresearch@gmail.com}% <-this % stops a space
\thanks{S. Islam and M. Qaraqe are with the College of Science and Engineering, Hamad Bin Khalifa University Doha, Qatar}% <-this % stops a space
\thanks{M. Erraguntla is with the Department of Industrial and Systems Engineering, Texas A\&M University, College Station, TX 77843}% <-this % stops a space
\thanks{M. Abdul-Ghani is with the UT Health, San Antonio, TX 78229}% <-this %
\thanks{Q. Abbasi is with the School of Engineering, University of Glasgow, UK}% <-this % stops a space
\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Communications Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
  We present a prediction model that assesses the future risk of developing type 2 diabetes mellitus (T2DM). We used the oral glucose tolerance test (OGTT) data collected from a group of 1,551  healthy subjects to construct a machine learning model employing the support vector machines. We trained and validated the models on the data obtained from the second cohort of the San Antonio Heart Study, using four features derived from the glucose measurements in the OGTT. We show that by only using the plasma glucose features, a personsprediction accuracy of 97.23\% The results of the proposed scheme show an average validation accuracy of 97.23\% and a hit rate of 77.27\%. The results also show that the plasma glucose based features are the strongest predictors of the future development of T2DM.
\end{abstract}
%
% Note that keywords are not normally used for peer-review papers.
\begin{IEEEkeywords}
  Type 2 Diabetes prediction, machine learning, disease risk assessment, San Antonio heart study.
\end{IEEEkeywords}
%
% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle
%
%
%
\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
%
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
%
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
%
% Some journals put the first two words in caps:
% \IEEEP\citeARstart{T}{his demo} file is ....
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{T}{he} global incidence of diabetes was estimated at \num{422} million in the year \num{2014} and its prevalence among the adult population has seen an increase from \SI{4.7}{\percent} in the year \num{1980} to \SI{8.5}{\percent} in \num{2014} \cite{mathers_projections_2006}. In \num{2015} alone, an estimated \num{1.6} million deaths worldwide were attributed to diabetes. In addition, a diabetic patient is at a greater risk of developing cardiovascular diseases, visual impairment and limb amputations, as compared to a non-diabetic person. Due to the substantial socio-economic burdens that are associated with diabetes, its early detection, intervention and prevention has become a worldwide top-level health concern.
%
%
%%% TALK ABOUT GLUCOSE TOLERANCE
The presumption of delaying or even preventing the future development of future diabetes is backed by experimental evidence \cite{tuomilehto2001prevention}, provided the person undergoes a lifestyle change that includes managing diet and incorporating exercise, and adheres to a pharmacological treatment. Moreover, one of the indicators of the early stages of diabetes is the impaired glucose tolerance (IGT) in which the blood sugar level rises beyond the normal levels defined by the World Health Organization (WHO) and the American Diabetes Association (AMA). Oral glucose tolerance test (OGTT) is recommended by the WHO as a diagnostic tool in which the prevalence of diabetes is indicated by an abnormally high blood glucose level at the \SI{2}{\hour}. It can also assist in screening the individuals that have IGT, and are at an increased risk of developing diabetes in the future. In an OGTT, the blood glucose and insulin levels are periodically recorded in a \SI{2}{\hour} period from a person that has undergone an overnight fast and administered a standard concentration of glucose. The outcomes of the OGTT provide useful information on the IGT, as well as any impairment in the insulin function of the body.

On the contrary, studies have indicated that only \SI{50}{\percent} of subjects with IGT went on to develop diabetes within a span of \num{10} years \cite{shaw_impaired_1999, writing_committee_impaired_2002}. Furthermore, long-term population studies have shown that around \SI{50}{\percent} future diabetic subjects did not exhibit IGT at all \cite{abdul-ghani_what_2007}. Previously, it has been shown that compared to the IGT, the glucose concentrations at the \SI{1}{\hour} and \SI{2}{\hour} intervals in an OGTT correlate more to the future diabetes risk \cite{abdul2009fasting,abdul-ghani_shape_2010,abdul-ghani_plasma_2009}.

Research studies that assess diabetes can be broadly categorized into two themes, first of which deals with the objective to detect any undiagnosed state of diabetes, and the second that aims to identify the person that are high risk of developing diabetes in the future \cite{noble2011risk}. The clinical significance of such investigations depends upon the data collection methods. Certain studies rely on collecting socio-demographic characteristics such as age, ethnicity, body mass index (BMI) and genealogical information through conducting population surveys, and then assign a probability to individuals of having diabetes \cite{Heikes1040,Glumer727}.  However, such self-assessment techniques can often be misleading and can not be relied upon. On the other hand, the outcomes of the diabetes related studies that involve physiological data such as blood samples collected in a laboratory environment provide an accurate clinical insight. We can further divide these types of enquiries into two types, namely, the screening  of undiagnosed diabetes, and the future prediction of diabetes. The former category has seen an increased amount of research interest in the last ten years. Using statistical and machine learning techniques, various researchers have developed \emph{risk models} for diabetes screening. In \cite{barakat_intelligible_2010}, support vector machine (SVM) framework was employed for the diagnosis of diabetes that also incorporated a tree-based decision making algorithm. A rule-based ensemble method combining SVM and random forest (RF) classifiers was used to detect diabetes in \cite{han_rule_2015} which provided an added comprehensibility of the classification mechanism.

Previous investigations designed to identify individuals at high risk of developing type 2 diabetes in future included San Antonio Diabetes prediction model (SADPM) \cite{stern2002identification} where a logistic regression model was constructed using a subject's physiological parameters such as systolic blood pressure and cholesterol level. The underlying causes of type 2 diabetes in the form insulin resistance and insulin secretion were studied to develop a prediction model in \cite{abdul-ghani_what_2007}. In another study,  multivariate logistic models using the plasma glucose values measured in the OGTT were used to predict the future risk of developing type 2 diabetes \cite{abdul-ghani_two-step_2011,abdul2009fasting}.

In this paper, we propose to develop a future diabetes prediction model by first identifying the variables computed from the OGTT that strongly correlate to future diabetes and then develop a support vector machine prediction model using these feature variables. For this purpose, we use the OGTT data generated from the population-based, epidemiological study, the San Antonio Heart Study (SAHS) \cite{burke_rapid_1999, lorenzo_trend_2006}.
%
\section{Materials and Methods}
%
\subsection{San Antonio Heart Study}
%
We developed the diabetes prediction models using the data extracted from a population-based epidemiological study, the San Antonio Heart Study (SAHS). The aim of the SAHS was to assess the risk factors of diabetes and cardiovascular diseases \cite{burke_rapid_1999, lorenzo_trend_2006}, for which \num[group-minimum-digits=4, group-separator = {,}]{5158} men and non-pregnant women of Mexican-American and non-Hispanic white residents of San Antonio, Texas were recruited. The age of the subjects at the time of recruitment was between \num{25} and \num{64} years. As a part of the data collection, the blood glucose and insulin levels were recorded during an oral glucose tolerance test (OGTT), which measures the subject's body response to a standard \SI{75}{\gram} dose of glucose after fasting overnight. The OGTT was performed both at the baseline and follow-up phases of the study, which had an average span of \num{7.5} years. The SAHS subjects were recruited in \num{2} cohorts, the first during the period \num{1979} to \num{1982}, and the second from \num{1984} to \num{1988} \cite{haffner_hyperinsulinemia_1986}. The reassessment during the follow-up period took place during the years \num{1987} to \num{1990} for the first cohort, and \num{1991} to \num{1996} for the second cohort. For the future T2DM prediction problem, we construct the machine learning model using the data from the second cohort, in which the plasma glucose and insulin levels of \num[group-minimum-digits=4, group-separator = {,}]{1496} of healthy subjects were recorded during the OGTT at times \num{0}, \num{30}, \num{60} and \num{120} minutes in the baseline evaluation. During the course of the study, a total of \num{171} subjects developed T2DM within which \num{10} subjects also reported of at least one cardiovascular event such as a heart attack, stroke or angina.

At the follow-up assessment, the participants were classified as having type 2 diabetes (T2D), cardiovascular disease (CVD) or normal. For T2D diagnosis, the WHO criteria, defining fasting glucose level \SI[round-mode = off,group-separator = {,}]{\ge 126}{\milli\gram\per\deci\liter} or 2-hour glucose level \SI[round-mode = off,group-separator = {,}]{\ge 200}{\milli\gram\per\deci\liter} was followed \cite{wei_effects_1998}. Any participant reportedly taking anti-diabetic medications was also classified as diabetic. For CVD classification, any cardiovascular event such as a heart attack, stroke or angina reported by the participant, was considered as an identifier. Table \ref{tab:patients} outlines the distribution of patient classification used in this study. In order to construct a binary classifier for this study, the subjects categorized under the `{DMI}' and `{DMI+CVD}' were encoded as the positive class whereas the `{Healthy}' labels were the negative class. We restricted the classification to only two classes and the samples with the label `{CVD}' were ignored.
%
\begin{table}[!h]
  \centering
  \renewcommand{\arraystretch}{1.3}
  \caption{The classification of the \num[group-minimum-digits=4, group-separator = {,}]{1496} subjects used in this study}
  % \vspace{-4mm}%Put here to reduce too much white space after table
  \centering
  \begin{tabular}{c c c c}
    \toprule
    Healthy &  DMI & CVD & DMI+CVD\\
    \midrule \midrule
    \num[group-minimum-digits=4, group-separator = {,}]{1,281} & \num{161} & \num{44} & \num{10}\\
    \SI{85.63}{\percent} & \SI{10.76}{\percent} & \SI[round-precision=3]{2.94}{\percent} & \SI[round-precision=2]{0.67}{\percent} \\
    \bottomrule
  \end{tabular}
  \label{tab:patients}
\end{table}
%
\subsection{Preparation of the Data}
%
The dataset included the glucose and insulin values recorded at the baseline, \num{30}, \num{60} and \num{120} minute intervals, and a distribution of these values marked by the follow-up labels of `healthy' and `diabetic' is shown in Fig. \ref{fig:ogtt_mean}. Moreover, the socio-demographic information such as age, ethnicity and body-mass index (BMI) was also part of the dataset. From the glucose and insulin measurements, we computed the slope and area under the curve between all the possible combinations  of a pair of readings. In addition, we also calculated parameters such as the insulinogenic (ratio of insulin and glucose slopes between any two time intervals) and Matsuda indices, as defined in \cite{Matsuda1462,abdul-ghani_plasma_2009}. These variables have shown a good efficacy of diabetes prediction in previous studies \cite{abdul-ghani_what_2007,abdul-ghani_plasma_2009}, since they are used to quantify the amount of insulin required by the body to maintain healthy glucose levels. In total we prepared \num{68} features for the classification problem and removed the rows that had missing entries or contained zero or infinite values.
%
\begin{figure*}
  \centering
  \begin{subfigure}[b]{\columnwidth}        %% or \columnwidth
    \centering
    \includegraphics[width=.9\linewidth]{boxpl_glucose.tex}
    \label{fig:glucose}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{\columnwidth}        %% or \columnwidth
    \centering
    \includegraphics[width=.9\linewidth]{boxpl_Insulin.tex}
    \label{fig:insulin}
  \end{subfigure}
  \caption{Box plots of glucose and insulin measurements for healthy and diabetic subjects.}
  \label{fig:ogtt_mean}
\end{figure*}
%
The dataset was then partitioned into training and validation sets. As can be observed from the Table \ref{tab:patients}, the SAHS dataset is intrinsically imbalanced with the class distribution skewed toward the majority class with a ratio of 7.5:1. The minority class of diabetic subjects was defined as the positive class with a label of \num{1}, whereas the majority class consisting of healthy persons was termed as the negative class marked by a \num{-1} label.
%
%
\subsection{Machine Learning Framework}
%
We developed a supervised learning scheme in which the classifier output labels were obtained from the follow-up data, and the support vector machine (SVM) technique was used to construct  the future diabetes prediction framework. The SVM works on the principle of \emph{structural risk minimization} (SRM) in which the goal is to develop a model from the given training data such that it generalizes well to new datasets and minimizes the empirical risk associated with misclassification of samples in the training set \cite{vapnik_nature_2000,vapnik2015uniform}. For a binary classification problem, the model constructed by the SVM finds a decision boundary or a separating hyperplane which aims to minimize the overlapping between the two classes in the training set. For problems that may not be amenable to linear separation between the two classes, the SVM technique is very attractive due to fact that the input feature space is first transformed to a higher dimension and then a linear boundary is determined, which generally gives better training performance \cite{friedman2001elements}. Let us consider a training data $(x_1, y_1)$,$(x_2, y_3)$,...,$(x_k, y_k)$ of $k$  pairs containing $x_i \in \mathbb R^N$ features and the binary classes $y_i \in {-1, 1}$. The SVM approach transforms the input features using a nonlinear mapping $\Phi : x \mapsto \phi(x)$ into a higher dimension space $\mathbb R^P$, where in general $P \gg N$. Due to the transformation, the classes can then be separated using a linear decision boundary in the enlarged space. The non-linear SVM classifier $\mathcal F$ is expressed in terms of the higher dimensional hyperplane,
%
\begin{equation}
  \mathcal F = \mathrm{sign} \left( \phi(x)^T \beta + \beta_0 \right).
  \label{eq:classifier}
\end{equation}
%
When the classes may not be completely separable, introducing a slack variable $\zeta$ in the higher dimension space $\mathbb R^P$ is a common practice which allows for the classifier output in \eqref{eq:classifier} to be on the incorrect side of the margin.
Therefore, in order to find the optimal separating hyperplane that maximizes the distance $M$ from the boundary for all the points, and bounds the value of $\sum_{i}{\zeta_i}$ and in turn misclassification rate, we introduce the convex optimization problem,
%
\begin{equation}
  \min_{\beta, \beta_0} \frac{1}{2} \norm{\beta}^2 + C\sum_{i=1}^{P} \zeta_i,
  \label{eq:min}
\end{equation}
%
with the nonlinear constraints $y_i (\phi(x_i)^T \beta + \beta_0) \ge 1 - \zeta_i \quad \forall i $ and $ \zeta_i \ge 0$, and the coefficient $C$ is termed as the cost parameter which decides the rigidity of the margin of the classifier. The solution of \eqref{eq:min} can be computed using the Lagrange primal objective function \cite{friedman2001elements},
%
\begin{multline}
  \mathcal L_{\mathrm{p}} = \dfrac{1}{2} \norm{\beta}^2 + C \sum \limits_{i =1}^{P} (1 - \mu_i)\zeta_i \\
  - \sum \limits_{i =1}^{P} \alpha_i \left[ y_i \left( \phi(x_i)^T \beta + \beta_0 \right) - (1 - \zeta_i) \right].
  \label{eq:l_primal}
\end{multline}
%
By minimizing $\mathcal L_{\mathrm{primal}}$ with respect to $\beta$, $\beta_0$, and $\zeta_i$, we get the corresponding dual form of the Lagrange function,
%
\begin{equation}
  \mathcal L_{\mathrm{dual}} = \sum \limits_{i = 1}^{P} \alpha_i - \frac{1}{2} \sum \limits_{i = 1}^{P} \sum_{i' = 1}^{P} \alpha_i \alpha_{i'} y_i y_{i'} \langle \phi(x_i), \phi(x_{i'}) \rangle
  \label{eq:l_dual}
\end{equation}
%
subject to $0 \le \alpha_i \le C$ and $\sum_{i} \alpha_i y_i = 0$ and the constraints, $\zeta_i, \mu_i \ge 0 \forall i$. The nonzero coefficients $\alpha_i$ and $\beta_0$ are determined using \eqref{eq:classifier}. As the dimension of the input feature space goes up, the computation of the mapping $\Phi$ gets excessive in complexity. With the introduction of a kernel,
%
\begin{equation}
  \mathcal K(x,x') = \langle \phi(x), \phi(x') \rangle,
  \label{eq:kernel}
\end{equation}
%
we can compute the inner product on \eqref{eq:l_dual} without  computing the mapping $\Phi$ \cite{scholkopf2002learning}, which becomes computationally expensive as the dimension of the input feature space increased. In this paper, we used the Gaussian radial basis function,
%
\begin{equation}
  \mathcal K(x,x') = \exp {\left(- \frac{\norm{x -x'}^2}{2\sigma^2}\right)}
  \label{eq:rbf}
\end{equation}
%
as the kernel where $\sigma$ is a free parameter. During the training, we tuned the values of the parameters $C$ and $\gamma$ through a grid search to obtain the optimal performance of the SVM.

For a linear variant of the SVM, the classifier can expressed without any coordinate mappings,
%
\begin{equation}
  \mathcal F^{\mathrm{lin}} = \mathrm{sign} \left( x^T \beta^{\mathrm{lin}} + \beta^{\mathrm{lin}}_0 \right).
  \label{eq:lin_classifier}
\end{equation}
%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%55%%%
\subsection{Feature Selection}
%
Before constructing the SVM model to predict the future risk of diabetes, we aim to find the most effective subset of the features in terms of the relevance to the classifier output. This process greatly reduces the computational cost during the model development by reducing the feature space dimension and also dispense useful scientific insight in to the classification problem. We performed a two-step feature selection, where first ten features that correlated the most to the classifier target class were shortlisted. In the second step, we ranked the shortlisted features by evaluating the accuracy through SVM classification and selected only the four best features.

In order to define the relevance between the feature and the class labels, consider a feature $x$ in the input feature space $\mathbb R^N$ as a continuous random variable and the class label $y$ as a discrete random variable. Their relationship can be described in terms of the mutual information, $\mathcal I$ defined as \cite{ross2014mutual}:
%
\begin{equation}
  \mathcal I(x, y) = - \int p_i \ln p_i \diff x - \sum \limits_j p_j\ln p_j + \sum_j \int p_{ij}\ln p_{ij} \diff x,
  \label{eq:MI}
\end{equation}
%
where $p_{i}$, and $p_{j}$ are the probabilities of the random variables $x$ and $y$ taking a particular value $x_i$ and $y_j \in (-1,1) \, \forall j$ respectively. The term $p_{ij}$ denotes the joint probability $P\{x= x_i, y=y_j\}$. The three terms in \eqref{eq:MI} represent the continuous, discrete and joint entropies of the random variables in the respective order. The features that are most relevant to the class label are the ones that individually yield the maximum $\mathcal I$. However, a drawback of pursuing this approach is that the selected features may be mutually correlated, and having a redundant list of shortlisted features only adds to the computational cost of the classifier without necessarily improving its performance. Even more so, the addition of extra features commonly result in the deterioration of the classifier performance \cite{trunk1979problem}. Therefore, an instinctive way forward is to keep only one feature from a correlated set of features that provides similar relevance information, and discard the remaining features from the set $\mathbb X$. We follow the minimal-redundancy-maximal-relevance (mRMR) algorithm \cite{mRMR}, that selects the features, that not only yield the maximal mutual information \eqref{eq:MI} with respect to the class label, but minimizes the mutual correlation among the features expressed in terms of redundancy $\mathcal R$ as:
%
\begin{equation}
  \mathcal R(\mathbb X) = \sum \limits_{x_i, x_j \in \mathbb{X}} \mathcal {I}(x_i, x_j).
  \label{eq:R}
\end{equation}
%
where $\mathcal I$ follows its definition in \eqref{eq:MI}. By minimizing $\mathcal R$, the mRMR framework selects a set of mutually exclusive features that are most relevant to the class label. Here, we first shortlist a set of ten features that are strong predictors of the future development of type 2 diabetes, on the basis of yielding maximum $\mathcal{I}$ with respect to the diabetic class. The application of the mRMR algorithm produces the features that are listed in Table \ref{tab:ranked} that are ranked in order of their relevance. The prefixes $AuC$ and $Sl$ denote the area under the curve and slope respectively, and the OGTT time interval corresponding to the feature appears in the subscripts.
%
\begin{table}[!h]
  \centering
  \renewcommand{\arraystretch}{1.3}
  \caption{List of ten most relevant features ranked by the mRMR algorithm}
  % \vspace{-4mm}%Put here to reduce too much white space after table
  \centering
  \begin{tabular}{c c}
    \toprule
    Rank &  Feature\\
    \midrule \midrule
    1 & AuC-Glu\textsubscript{0-120}\\
    2 & Sl-Glu\textsubscript{120-0}\\
    3 & Sl-Glu\textsubscript{120-60}\\
    4 & Sl-Glu\textsubscript{60-0}\\
    5 & Sl-Glu\textsubscript{30-0}\\
    6 & AuC-Glu\textsubscript{60-120}\\
    7 & PG\textsubscript{0}\\
    8 & PG\textsubscript{120}\\
    9 & PG\textsubscript{60} \\
    10 & AuC-Glu\textsubscript{30-120}\\
    \bottomrule
  \end{tabular}
  \label{tab:ranked}
  % \vspace{4mm}%Put here to reduce too much white space after table
\end{table}
%

In the second phase, we further refined the number of variables to four by only selecting the ones which provided the best performance using the SVM classification scheme with the parameters $C$ and $\gamma$ preconfigured to a value of \num{1}. For this purpose, we employed the accuracy achieved in the validation set as the evaluation criterion. Table \ref{tab:best_features} shows the mean validation accuracies of the variables which is  obtained by performing \num{100} iterations of the SVM classifier supplied with only one variable at a time.
%++
\begin{table}[!htbp]
  \centering
  \renewcommand{\arraystretch}{1.3}
  \caption{Average performance of the individual features gauged by the accuracy}
  \sisetup{
  round-mode = places,
  round-precision = 3
  }%
  \centering
  \begin{tabular}{c c c}
    \toprule
    Features &  Mean Accuracy (SD)\\
    \midrule \midrule
    \rowcolor{green!25} AuC-Glu\textsubscript{60-120}	& \num{0.973085106382978}	(\num{0.0133930614140817})\\
    \rowcolor{green!25} PG\textsubscript{120}	& \num{0.971276595744680}	(\num{0.0150827692653638})\\
    \rowcolor{green!25} PG\textsubscript{60}	& \num{0.967234042553191}	(\num{0.0222062121383667})\\
    \rowcolor{green!25} AuC-Glu\textsubscript{0-120}	& \num{0.958404255319148}	(\num{0.0192510470888241})\\
    AuC-Glu\textsubscript{30-120}	& \num{0.950212765957446}	(\num{0.0176257382363412}) \\
    Sl-Glu\textsubscript{60-0}	& \num{0.946063829787233}	(\num{0.0254033135631271})\\
    Sl-Glu\textsubscript{120-0}	& \num{0.931276595744681}	(\num{0.0261206217319048})\\
    PG\textsubscript{0}	& \num{0.8159}	(\num{0.0394})\\
    Sl-Glu\textsubscript{120-60}	& \num{0.7632}	(\num{0.0499})\\
    Sl-Glu\textsubscript{30-0}	& \num{0.7452}	(\num{0.044})\\
    \bottomrule
  \end{tabular}
  \label{tab:best_features}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Data Experiments}
%
In this paper, we employed the non-linear SVM \eqref{eq:classifier} in the form of radial basis functions (RBF) \eqref{eq:rbf} since the classes can not be linearly separated directly as observed in Fig. \ref{fig:ogtt_mean}. We also used the linear variant of the SVM \eqref{eq:lin_classifier} to compare the classifier performances. Moreover, due to the unbalanced nature of the dataset, we conducted two experiments in the preprocessing phase. In the first the dataset was balanced where we randomly undersampled the majority class, and  took \num{160} instances from each class for the training. In the second experiment, we retained the class ratio of the data set and took \num[group-minimum-digits=4, group-separator = {,}]{1360} samples to generate the training set that contained \num{160} and \num[group-minimum-digits=4, group-separator = {,}]{1200} instances of the diabetic and healthy classes respectively. In order to ensure that the model remained unbiased and generalized well to new data, we performed \num{10}-fold cross-validation during the training and the performance obtained was averaged over all the \num{10} folds. All the experiments were carried out using the statistical and machine learning toolbox of Matlab and the data was normalized prior to the training. The optimal hyperplane parameters $C$ and $\sigma$ in \eqref{eq:min} and \eqref{eq:rbf} respectively were determined through a grid search with a view to maximize the classifier hit rate defined as,
%
\begin{equation}
  \mathrm{Hit~Rate} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}},
\end{equation}
%
where TP and FN refer to the number of correctly and incorrectly classified diabetic subjects respectively.
%
%
\begin{table}[!t]
  \centering
  \tiny
  \renewcommand{\arraystretch}{1.3}
  \caption{Mean Training performance of the classifiers}
  \centering
  \begin{tabularx}{.95\columnwidth}{c c c c}
    \toprule
    &  Accuracy \textpm ~SD & Hit Rate \textpm ~SD & Specificity \textpm ~SD\\
    \midrule \midrule
    Linear SVM (Balanced) & \SI{78.73 \pm 1.4}{\percent} & \SI{77.77 \pm 1.2}{\percent} & \SI{79.69 \pm 2.3}{\percent} \\
    Linear SVM (Unbalanced) & \SI{79.74 \pm 0.43}{\percent} & \SI{77.81 \pm 0.9}{\percent} & \SI{80.00 \pm 0.4}{\percent} \\
    SVM-RBF (Balanced) & \SI{78.95 \pm 1.9}{\percent} & \SI{79.07 \pm 2.2}{\percent} & \SI{78.84 \pm 2.6}{\percent} \\
    SVM-RBF (Unbalanced) & \SI{78.51 \pm .8}{\percent} & \SI{78.13 \pm 1.3}{\percent} & \SI{78.57 \pm 1.0}{\percent} \\
    \bottomrule
  \end{tabularx}
  \label{tab:training}
  % \vspace{-8mm}%Put here to reduce too much white space after table
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Results and Discussion}
%
%
In order to correctly predict the future diabetes subjects, the model was trained to maximize the recall or hit rate during the training. To train the predictor model, we used four features, all of which were derived from the blood glucose measurements. Table \ref{tab:training} presents the mean training performance of the linear and non-linear SVM classifiers obtained over \num{100} trials. We used the definition of accuracy as the ratio of number of correctly classified subjects to the total number of subjects, whereas the specificity was the ratio of the correctly classified healthy subjects to the total number of healthy subjects. Most notably, the similarity in the performance for balanced and unbalanced training routines demonstrates that the model in the latter case unbiased toward the majority class. The optimal hyperplane parameters corresponding to the Matlab arguments `BoxConstraint' and `Gamma', were respectively assigned the values of \num{1.0} and \num{5.0}. For the linear version of the SVM, an average of \num{250} and \num[group-minimum-digits=4, group-separator = {,}]{1114} support vectors were used to construct the hyperplane for the balanced and unbalanced datasets respectively. On the other hand, the corresponding values were \num{278} and \num[group-minimum-digits=4, group-separator = {,}]{1198} for the nonlinear SVM with the RBF as the kernel. It should be noted that the difference in the dimensionality of the hyperplanes between the two variants of the SVM is not large, which indicates that the discriminating power of the features used.

Table \ref{tab:validation} displays the validation performance of the classifiers. All the model were validated on a hold-out set \num{100} times set, in which each iteration resulted in a randomly generated set of \num{11} diabetic and \num{83} healthy samples, that were not part of the training data. The best mean performance of \SI{98.51}{\percent} accuracy and  recall of \SI{87.27}{\percent} was obtained from the nonlinear SVM with the RBF kernel. The standard deviation of the two metrics along \num{100} iterations was \SI{1.40}{\percent} and \SI{11.50}{\percent} respectively. We also compared the results of our approach with two other techniques that used the SAHS dataset. The logistic regression based SADPM based on
% LIN BAL
% ans =
%     0.7855    0.7815    0.7873    0.7867
% ans =
%     0.0138    0.0153    0.0139    0.0139
% ans =
%     0.7761    0.7780    0.7777    0.7764
% ans =
%     0.0126    0.0130    0.0118    0.0113
% ans =
%     0.7948    0.7849    0.7969    0.7970
% ans =
%     0.0211    0.0229    0.0228    0.0231
%
% LIN UNBAL
% ans =
%     0.7973    0.7868    0.7974    0.7969
% ans =
%     0.0045    0.0038    0.0043    0.0045
% ans =
%     0.7774    0.7778    0.7781    0.7776
% ans =
%     0.0093    0.0090    0.0092    0.0090
% ans =
%     0.7999    0.7880    0.8000    0.7994
% ans =
%     0.0044    0.0039    0.0041    0.0044%
% SVM-RBF BAL
% ans =
%     0.7881    0.7852    0.7895    0.7869
% ans =
%     0.0175    0.0175    0.0187    0.0173
% ans =
%     0.7843    0.7864    0.7907    0.7798
% ans =
%     0.0201    0.0163    0.0216    0.0198
% ans =
%     0.7918    0.7841    0.7884    0.7941
% ans =
%     0.0252    0.0258    0.0261    0.0246
%
% SVM-RBF UNBAL
% ans =
%     0.7851    0.7791    0.7845    0.7790
% ans =
%     0.0075    0.0094    0.0060    0.0070
% ans =
%     0.7813    0.7800    0.7831    0.7850
% ans =
%     0.0132    0.0097    0.0186    0.0119
% ans =
%     0.7857    0.7790    0.7847    0.7782
% ans =
%     0.0096    0.0114    0.0079    0.0077
%

%
%
% LIN BAL
% ans =
%     0.9711    0.9729    0.9715    0.9710
% ans =
%     0.0147    0.0140    0.0146    0.0146
% ans =
%     0.7527    0.7682    0.7564    0.7518
% ans =
%     0.1253    0.1194    0.1245    0.1252
% ans =
%      1     1     1     1
% ans =
%      0     0     0     0
%
% LIN UNBAL
% ans =
%     0.9661    0.9719    0.9655    0.9663
% ans =
%     0.0183    0.0154    0.0188    0.0180
% ans =
%     0.7664    0.7682    0.7645    0.7691
% ans =
%     0.1287    0.1268    0.1280    0.1282
% ans =
%     0.9925    0.9989    0.9922    0.9924
% ans =
%     0.0097    0.0039    0.0099    0.0096
%
% RBF BAL
% ans =
%     0.9761    0.9760    0.9762    0.9754
% ans =
%     0.0151    0.0143    0.0148    0.0148
% ans =
%     0.7955    0.7945    0.7964    0.7900
% ans =
%     0.1295    0.1223    0.1267    0.1264
% ans =
%      1     1     1     1
% ans =
%      0     0     0     0
%
% RBF UNBAL
% ans =
%     0.9819    0.9830    0.9851    0.9840
% ans =
%     0.0167    0.0135    0.0135    0.0144
% ans =
%     0.8545    0.8545    0.8727    0.8636
% ans =
%     0.1227    0.1150    0.1150    0.1231
% ans =
%     0.9988    1.0000    1.0000    1.0000
% ans =
%     0.0038         0         0         0
%
\begin{table}[!t]
  \centering
  \tiny
  \renewcommand{\arraystretch}{1.3}
  \caption{ Validation performance classifiers}
  \centering
  \begin{tabularx}{.95\columnwidth}{c c c c}
    \toprule
    &  Accuracy \textpm ~SD & Hit Rate \textpm ~SD & Specificity \textpm ~SD\\
    \midrule \midrule
    Linear SVM (Balanced) & \SI{97.29 \pm 1.4}{\percent}  & \SI{76.82 \pm 11.9}{\percent}  & \SI{100}{\percent} \\
    Linear SVM (Unbalanced) & \SI{97.19 \pm 1.5}{\percent} & \SI{76.82 \pm 12.70 }{\percent}  & \SI{99.89 \pm 0.4}{\percent} \\
    SVM-RBF (Balanced) & \SI{97.62 \pm 1.5}{\percent} & \SI{79.64 \pm 12.70}{\percent}  & \SI{100}{\percent} \\
    SVM-RBF (Unbalanced) & \SI{98.51 \pm 1.40}{\percent} & \SI{87.27 \pm 11.50}{\percent}  & \SI{100}{\percent} \\
    Two-step Approach \cite{abdul-ghani_two-step_2011} & \SI{77.43}{\percent} & \SI{77.70}{\percent} & \SI{77.40}{\percent} \\
    SADPM \cite{stern2002identification} & \SI{56.329}{\percent} & \SI{88.80}{\percent} & \SI{52.00}{\percent} \\
    \bottomrule
  \end{tabularx}
  \label{tab:validation}
  % \vspace{-8mm}%Put here to reduce too much white space after table
\end{table}
%
%
% cc_AuG60_120,acc_PG120, acc_PG60,...
% acc_AuG0_120, acc_AuG30_120,...
% acc_SlG60_0, acc_SlG120_0
% 0.973085106382978
% 0.971276595744680
% 0.967234042553191
% 0.958404255319148
% 0.950212765957446
% 0.946063829787233
% 0.931276595744681
%
% 0.0133930614140817
% 0.0150827692653638
% 0.0222062121383667
% 0.0192510470888241
% 0.0176257382363412
% 0.0254033135631271
% 0.0261206217319048
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Conclusion}
%
%
In this paper, we developed a SVM prediction model to identify the persons that are an increased risk of developing type 2 diabetes in the future. We showed that a high prediction performance can be achieved by extracting information from a person's abnormally high blood glucose levels. The predictive power of our approach in terms of the accuracy and in particular the recall, is significantly greater than the similar techniques used previously.

A drawback of this approach is the specialized nature of the oral glucose tolerance test generating the requisite data, which is expensive and invasive in which samples of the person's blood samples are taken more than once. However, we believe that the cost incurred and the associated inconvenience is compensated by the critical information obtained as a result through which a physician can recommend intervention measures to reduce or avoid the chances of having diabetes in the future.
%
%
%
%
\section*{Acknowledgment}
%
This publication was made possible by NPRP grant number NPRP 10-1231-160071 from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors.
%
%
% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
\newpage
\fi
% \balance
\bibliographystyle{IEEEtran}
% \bibliographystyle{ieeetr}
% \atColsEnd{\vfil}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{References}
%
%
\begin{IEEEbiography}{Michael Shell}
  Biography text here.
\end{IEEEbiography}
%
% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{John Doe}
  Biography text here.
\end{IEEEbiographynophoto}
%
% insert where needed to balance the two columns on the last page with
% biographies
%\newpage
%
\begin{IEEEbiographynophoto}{Jane Doe}
  Biography text here.
\end{IEEEbiographynophoto}
%
% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}
